{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, kita bikin list of word vectornya dulu sebagai bentuk word embedding.\n",
    "Ini kita pake yang yg udh di train dari Glove, jadi basically ngk usah train word embedding lagi.\n",
    "Isinya 400 ribu kata, dengan dimensi vektor 50.\n",
    "Cmn kayanya perlu dihandle lagi word embeddingnya, karena feeling gua OOV-nya bakal banyak.\n",
    "Opsinya : 1. Train lagi word embeddingnya pake dataset 2. Normalisasi input 3. Handle OOV pake word similarity, cari yang paling deket tapi komputasinya gede karena bandingin sama 400k kata\n",
    "Sementara, biar simple kalo OOV langsung diset vektor UNKNOWN (yang ke-39999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita itung dulu rata-rata jumlah kata per tweet, buat nentuin maximum sequence length, biar sama semua dimensi embeddingnya per input tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of opinion is 110\n",
      "The total number of words is 1541\n",
      "The average number of words is 14.00909090909091\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "numWords = []\n",
    "for i in range(0, len(df)):\n",
    "    row = df.iloc[i]\n",
    "    numWords.append(len(row['opinion'].split()))\n",
    "\n",
    "print('The total number of opinion is', len(df))\n",
    "print('The total number of words is', sum(numWords))\n",
    "print('The average number of words is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisasi histogram jumlah kata per tweet buat di laporan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npMYZBJLUuP8HknzwGkSWv4sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d212e61f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, bins=50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dimensi word embedding 50 sesuai Glove, set maxSeqLength 20 ambil sedikit di atas rata-rata (14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions for each word vector\n",
    "# numDimensions = 300\n",
    "# Harusnya 50, ngk ngerti kenapa dia bikin 300\n",
    "numDimensions = 50\n",
    "maxSeqLength = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing : 1. Lowercase 2. Buang tanda baca dan karakter2 ngk penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ubah dataset jadi matrix vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "ids = np.zeros((len(df), maxSeqLength+1), dtype='int32')\n",
    "fileCounter = 0\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    print(i)\n",
    "    indexCounter= 0\n",
    "    row = df.iloc[i]\n",
    "    line = row['opinion']\n",
    "    sentiment = row['sentiment']\n",
    "    if (str(sentiment) == 'positif'):\n",
    "        ids[fileCounter][maxSeqLength] = -1 #kelas positif\n",
    "    else:\n",
    "        ids[fileCounter][maxSeqLength] = -2 #kelas negatif\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        if('http' not in word):\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "    fileCounter = fileCounter + 1 \n",
    "        \n",
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "np.save('idsMatrixBallon', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    47, 399999,     79,    111,  18471,      5,     32,   4943,\n",
       "           378,    236,     68,     10,   7035,     79,   8160,      0,\n",
       "             0,      0,      0,      0,     -1],\n",
       "       [    47, 399999,     79,    111,      5, 399999,     32,   4943,\n",
       "           378,    236,     68,     10,   7035,     79,  19087,      0,\n",
       "             0,      0,      0,      0,     -1],\n",
       "       [ 20956,  46501,   1733,  19087, 103285,  19087, 399999, 399999,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,     -1],\n",
       "       [  1716,    182,     62,     61, 201534,    493,    398,     10,\n",
       "          8160,      4,    320, 399999,   2715,    197,   9956,     18,\n",
       "            14,   8457,    197,    181,     -2],\n",
       "       [399999,     81,    269,   8160,     14,   6034,    113,      3,\n",
       "        399999,   1284,     46,    120,  12594,     17,   2145,    236,\n",
       "           186,      0,      0,      0,     -1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.load('idsMatrixBallon.npy')\n",
    "ids[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find a couple of helper functions that will be useful when training the network in a later step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "batchSize = 10\n",
    "\n",
    "def getTrainBatch(start):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        idx = i + batchSize*start\n",
    "        if(ids[idx][maxSeqLength] == -1):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[idx][0:maxSeqLength]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch(start):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        idx = i + batchSize*start\n",
    "        if(ids[idx][maxSeqLength] == -1):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[idx][0:maxSeqLength]\n",
    "    return arr, labels\n",
    "\n",
    "# arr, labels = getTrainBatch(0)\n",
    "# arr[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparam dari RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 10\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder buat label (positif/negatif) sama input kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(10, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(10, 20) dtype=int32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "print(labels)\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "input_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(10, 20, 50) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors, input_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set jumlah lstm sama dropout. Masukin format data ke lstm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inisialisasi2 Awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variabel evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer : Adam, Output function : softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss dan accuracy visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 50)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right values for your hyperparameters is a crucial part of training deep neural networks effectively. You'll find that your training loss curves can vary with your choice of optimizer (Adam, Adadelta, SGD, etc), learning rate, and network architecture. With RNNs and LSTMs in particular, some other important factors include the number of LSTM units and the size of the word vectors.\n",
    "\n",
    "* Learning Rate: RNNs are infamous for being diffult to train because of the large number of time steps they have. Learning rate becomes extremely important since we don't want our weight values to fluctuate wildly as a result of a large learning rate, nor do we want a slow training process due to a low learning rate. The default value of 0.001 is a good place to start. You should increase this value if the training loss is changing very slowly, and decrease if the loss is unstable.  \n",
    "* Optimizer: There isn't a consensus choice among researchers, but Adam has been widely popular due to having the adaptive learning rate property (Keep in mind that optimal learning rates can differ with the choice of optimizer).\n",
    "* Number of LSTM units: This value is largely dependent on the average length of your input texts. While a greater number of units provides more expressibility for the model and allows the model to store more information for longer texts, the network will take longer to train and will be computationally expensive. \n",
    "* Word Vector Size: Dimensions for word vectors generally range from 50 to 300. A larger size means that the vector is able to encapsulate more information about the word, but you should also expect a more computationally expensive model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of the training loop is that we first define a Tensorflow session. Then, we load in a batch of reviews and their associated labels. Next, we call the session’s `run` function. This function has two arguments. The first is called the \"fetches\" argument. It defines the value we’re interested in computing. We want our optimizer to be computed since that is the component that minimizes our loss function. The second argument is where we input our `feed_dict`. This data structure is where we provide inputs to all of our placeholders. We need to feed our batch of reviews and our batch of labels. This loop is then repeated for a set number of training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the network in this notebook (which will take at least a couple of hours), we’ll load in a pretrained model.\n",
    "\n",
    "If you decide to train this notebook on your own machine, note that you can track its progress using [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). While the following cell is running, use your terminal to enter the directory that contains this notebook, enter `tensorboard --logdir=tensorboard`, and visit http://localhost:6006/ with a browser to keep an eye on your training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "iterations = 1100\n",
    "start = 0\n",
    "for i in range(iterations):\n",
    "    print(i)\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch(start);\n",
    "    if(start < (len(df)/batchSize)-1):\n",
    "        start = start + 1\n",
    "    else:\n",
    "        start = 0\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pretrained model’s accuracy and loss curves during training can be found below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis6.png)\n",
    "![caption](Images/SentimentAnalysis7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training curves above, it seems that the model's training is going well. The loss is decreasing steadily, and the accuracy is approaching 100 percent. However, when analyzing training curves, we should also pay special attention to the possibility of our model overfitting the training dataset. Overfitting is a common phenomenon in machine learning where a model becomes so fit to the training data that it loses the ability to generalize to the test set. This means that training a network until you achieve 0 training loss might not be the best way to get an accurate model that performs well on data it has never seen before. Early stopping is an intuitive technique commonly used with LSTM networks to combat this issue. The basic idea is that we train the model on our training set, while also measuring its performance on the test set every now and again. Once the test error stops its steady decrease and begins to increase instead, you'll know to stop training, since this is a sign that the network has begun to overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a pretrained model involves defining another Tensorflow session, creating a Saver object, and then using that object to call the restore function. This function takes into 2 arguments, one for the current session, and one for the name of the saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we’ll load some movie reviews from our test set. Remember, these are reviews that the model has not been trained on and has never seen before. The accuracy for each test batch can be seen when you run the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n",
      "Accuracy for this batch: 100.0\n"
     ]
    }
   ],
   "source": [
    "iterations = 11\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch(i);\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ronaldo is very good\n",
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter, word in enumerate(split):\n",
    "        if(indexCounter >= maxSeqLength):\n",
    "            break\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix\n",
    "\n",
    "inputText = input()\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "\n",
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "# predictedSentiment[0] represents output score for positive sentiment\n",
    "# predictedSentiment[1] represents output score for negative sentiment\n",
    "\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we went over a deep learning approach to sentiment analysis. We looked at the different components involved in the whole pipeline and then looked at the process of writing Tensorflow code to implement the model in practice. Finally, we trained and tested the model so that it is able to classify movie reviews.\n",
    "\n",
    "With the help of Tensorflow, you can create your own sentiment classifiers to understand the large amounts of natural language in the world, and use the results to form actionable insights. Thanks for reading and following along!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
